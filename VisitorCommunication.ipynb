https://github.com/iamjagdeesh/Data-Visualization-Assignments/blob/master/Week_4/Assignment%2B4.ipynb
https://scholarworks.smith.edu/cgi/viewcontent.cgi?article=1090&context=csc_facpubs
https://bib.dbvis.de/uploadedFiles/ukon103.pdf

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
df_fri = pd.read_csv('comm-data-Fri.csv')
df_sat = pd.read_csv('comm-data-Sat.csv')
df_sun = pd.read_csv('comm-data-Sun.csv')

df = pd.concat([df_fri, df_sat, df_sun])
print(df.head())
### EDA

### Identify groups with large volumes of communication and characterize their communication patterns.
# Count the number of sender
communication_counts = df['from'].value_counts()
dominant_communication_ids = communication_counts[communication_counts >= communication_counts.quantile(0.95)].index
dominant_communication_patterns = df[df['from'].isin(dominant_communication_ids)]

print(communication_counts)
print(dominant_communication_patterns.head())
### Characterize the communication patterns you see.
Find high Communication Volume.
outgoing_comm_counts = df['from'].value_counts()
incoming_comm_counts = df['to'].value_counts()
total_comm_counts = outgoing_comm_counts.add(incoming_comm_counts, fill_value=0)
high_volume_communicators = total_comm_counts.sort_values(ascending=False)

print(high_volume_communicators.head(10))
### Based on these patterns, what do you hypothesize about these IDs?
#### High Communication Volume IDs: 
Visitors with a high communication volume may include group leaders, event organizers, or influential individuals who actively engage with other visitors. They could be coordinating meetups, sharing event information, or facilitating group interactions.

#### Group Communication IDs:
IDs involved in frequent group communication may belong to visitors who are part of a close-knit group or traveling together. They could be friends, family members, or colleagues who communicate regularly during their visit to the park.

#### External Communication IDs:
IDs that frequently send messages to external parties might be visitors who maintain communication with people outside the park. They could be sharing their park experiences with friends or family who are not present at the event.

#### Communication Content Analysis: 
By analyzing the content of messages, we might identify certain visitor interests, preferences, or sentiments. For example, visitors discussing specific attractions or events might be expressing enthusiasm for those experiences.

#### Location-Based Communication IDs: 
Visitors who communicate more frequently in certain park areas might indicate their preferences for specific attractions or activities in those regions.

#### Communication Peaks: 
IDs involved in communication peaks during specific events or attractions might be visitors expressing excitement or sharing their experiences during those moments.

#### Interaction with Park Services IDs: 
Visitors who frequently communicate with park services might be seeking assistance, reporting issues, or making inquiries about park facilities.

#### Influence of Attractions IDs: 
IDs involved in increased communication after certain attractions might indicate visitors sharing their experiences or coordinating future plans.

#### Communication during Scott Jones Weekend IDs: 
Visitors who show distinct communication patterns during the Scott Jones Weekend might be those actively engaging with the event, attending Scott Jones shows, or discussing the memorabilia.

#### Group IDs: 
Based on group communication, we might identify visitor groups with common interests, demographics, or origins. These groups could be families, school groups, tour groups, etc.
df_fri = pd.read_csv('comm-data-Fri.csv')
df_fri.rename(columns={'from': 'originID', 'to': 'reciptID'}, inplace=True)
print(df_fri.head())
#### Large Volumes of Communication
# Concatenate 'originID' and 'reciptID' to form all unique communication groups
all_groups = pd.concat([df_fri['originID'], df_fri['reciptID']]).unique()

# Calculate communication volume for each visitor (originID)
communication_volume = {
    visitor: len(df_fri[(df_fri['originID'] == visitor) | (df_fri['reciptID'] == visitor)])
    for visitor in all_groups
}

# Set a threshold for large communication volumes (you can adjust this based on your criteria)
large_communication_threshold = 50

# Filter visitors with large communication volumes
large_communication_visitors = [visitor for visitor, volume in communication_volume.items() if volume >= large_communication_threshold]

# Print the visitors with large communication volumes and their communication volume
for visitor in large_communication_visitors:
    print(f"Visitor ID: {visitor}, Communication Volume: {communication_volume[visitor]}")

#### Group Communication IDs:
# Convert 'originID' and 'reciptID' columns to string type
df_fri['originID'] = df_fri['originID'].astype(str)
df_fri['reciptID'] = df_fri['reciptID'].astype(str)

# Group the data by 'originID' and count the number of unique recipients for each originID
group_communication_counts = df_fri.groupby('originID')['reciptID'].nunique()

# Filter the originIDs with multiple unique recipients (group communication IDs)
group_communication_ids = group_communication_counts[group_communication_counts > 1].index.tolist()

# Print the identified group communication IDs
print("Group Communication IDs:")
print(group_communication_ids)
#### External Communication IDs:
external_communication = df_fri[df_fri['reciptID'].str.contains('external', case=False, na=False)]
external_communication_ids = external_communication['originID'].unique()
print("External Communication IDs:")
print(external_communication_ids)
#### Communication Peaks
# Group communication data by timestamp to calculate communication volume per timestamp
communication_volume_per_timestamp = df_fri.groupby('Timestamp').size()

# Smooth the data using rolling average to reduce noise and highlight peaks
rolling_window = 20
smoothed_volume_per_timestamp = communication_volume_per_timestamp.rolling(window=rolling_window, min_periods=1).mean()

# Find communication peaks based on the smoothed data
communication_peaks = smoothed_volume_per_timestamp[smoothed_volume_per_timestamp > smoothed_volume_per_timestamp.mean()]

# Print the communication peaks and their corresponding timestamps
print("Communication Peaks:")
print(communication_peaks)
# Create a line chart to visualize communication volume and peaks over time
plt.figure(figsize=(12, 6))
plt.plot(communication_volume_per_timestamp.index, communication_volume_per_timestamp, label='Communication Volume')
plt.plot(smoothed_volume_per_timestamp.index, smoothed_volume_per_timestamp, color='red', label='Smoothed Volume')
plt.scatter(communication_peaks.index, communication_peaks.values, color='green', label='Communication Peaks')
plt.xlabel('Timestamp')
plt.ylabel('Communication Volume')
plt.legend()
plt.title('Communication Volume and Peaks over Time')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
#line chart to visualize communication peaks over time
plt.figure(figsize=(12, 6))
plt.scatter(communication_peaks.index, communication_peaks.values, color='green', label='Communication Peaks')
plt.xlabel('Timestamp')
plt.ylabel('Communication Volume')
plt.legend()
plt.title('Communication Peaks over Time')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

#line chart to visualize smoothed volume and peaks over time
plt.figure(figsize=(12, 6))
plt.plot(smoothed_volume_per_timestamp.index, smoothed_volume_per_timestamp, color='red', label='Smoothed Volume')
plt.scatter(communication_peaks.index, communication_peaks.values, color='green', label='Communication Peaks')
plt.xlabel('Timestamp')
plt.ylabel('Communication Volume')
plt.legend()
plt.title('Smoothed Volume and Communication Peaks over Time')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
#### Location-Based Communication IDs
# Calculate communication frequency from each location for each visitor
communication_frequency_by_location = df_fri.groupby(['originID', 'location']).size().reset_index(name='Frequency')

# Find the most frequently communicated location for each visitor
most_frequent_locations = communication_frequency_by_location.loc[communication_frequency_by_location.groupby('originID')['Frequency'].idxmax()]

print("Location-Based Communication IDs:")
print(most_frequent_locations[['originID', 'location', 'Frequency']])

#### Interaction with Park Services IDs
df_fri = pd.read_csv('comm-data-Fri.csv')
df_sat = pd.read_csv('comm-data-Sat.csv')
df_sun = pd.read_csv('comm-data-Sun.csv')

df = pd.concat([df_fri, df_sat, df_sun])
df.rename(columns={'from': 'originID', 'to': 'reciptID'}, inplace=True)
print(df.head())

import pandas as pd
# Convert 'originID' and 'reciptID' columns to string type
df['originID'] = df['originID'].astype(str)
df['reciptID'] = df['reciptID'].astype(str)

# Filter communication data to include interactions with park services (help desk)
park_services_interactions = df[
    (df['reciptID'] == 'help desk') |  # Visitors contacting help desk
    (df['originID'].str.contains('help desk', na=False))  # Help desk contacting visitors
]

# Find the unique originIDs involved in interactions with park services
park_services_ids = park_services_interactions['originID'].unique()


print(len(park_services_interactions))

# Print the IDs of visitors who interacted with park services
print("Interaction with Park Services IDs:")
print(park_services_ids)
#### Influence of Attractions IDs
# Group communication data by 'location' to calculate communication frequency for each attraction
attraction_communication_frequency = df_fri.groupby('location').size()

# Sort attractions by communication frequency in descending order
attraction_communication_frequency = attraction_communication_frequency.sort_values(ascending=False)

# Plot the influence of attractions on communication using a bar chart
plt.figure(figsize=(12, 6))
attraction_communication_frequency.plot(kind='bar')
plt.xlabel('Attraction')
plt.ylabel('Communication Frequency')
plt.title('Influence of Attractions on Communication')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

#### Communication during Scott Jones Weekend IDs
df_fri = pd.read_csv('comm-data-Fri.csv')
df_sat = pd.read_csv('comm-data-Sat.csv')
df_sun = pd.read_csv('comm-data-Sun.csv')
df = pd.concat([df_fri, df_sat, df_sun])

df.rename(columns={'from': 'originID', 'to': 'reciptID'}, inplace=True)
print(df.head())
# Define the time slots for Scott Jones stage shows on each day
friday_show_time = ('8:45', '11:35')
saturday_show_time = ('8:45', '11:35')
sunday_show_time = ('8:45', '11:35')

# Convert 'Timestamp' column to datetime format
df['Timestamp'] = pd.to_datetime(df['Timestamp'])

# Filter communication data during Scott Jones Weekend time slots
scott_jones_communication = df[
    (df['Timestamp'].dt.time >= pd.to_datetime(friday_show_time[0]).time()) &
    (df['Timestamp'].dt.time <= pd.to_datetime(friday_show_time[1]).time()) |
    (df['Timestamp'].dt.time >= pd.to_datetime(saturday_show_time[0]).time()) &
    (df['Timestamp'].dt.time <= pd.to_datetime(saturday_show_time[1]).time()) |
    (df['Timestamp'].dt.time >= pd.to_datetime(sunday_show_time[0]).time()) &
    (df['Timestamp'].dt.time <= pd.to_datetime(sunday_show_time[1]).time())
]

# Find the unique originIDs involved in communication during Scott Jones Weekend
scott_jones_communication_ids = scott_jones_communication['originID'].unique()

# Print the IDs of visitors who communicated during Scott Jones Weekend
print("Communication during Scott Jones Weekend IDs:")
print(scott_jones_communication_ids)

### Path analysis output by attraction for large groups.
df_fri = pd.read_csv('comm-data-Fri.csv')
df_sat = pd.read_csv('comm-data-Sat.csv')
df_sun = pd.read_csv('comm-data-Sun.csv')

df = pd.concat([df_fri, df_sat, df_sun])
print(df.head())
import pandas as pd
import matplotlib.pyplot as plt

# Define the criteria for a large group
large_group_threshold = 10
large_groups_df = df.groupby(['from', 'to']).filter(lambda x: len(x) >= large_group_threshold)
grouped_paths = large_groups_df.groupby(['from', 'to'])

group_paths = {}

for (sender, recipient), group in grouped_paths:
    path = list(group['location'])
    if sender not in group_paths:
        group_paths[sender] = {}
    group_paths[sender][recipient] = path

for sender, recipient_paths in group_paths.items():
    for recipient, path in recipient_paths.items():
        plt.figure(figsize=(10, 6))
        plt.plot(range(len(path)), path, marker='o')
        plt.title(f'Path Analysis: {sender} -> {recipient}')
        plt.xlabel('Step')
        plt.ylabel('Attraction')
        plt.xticks(range(len(path)), path, rotation=45)
        plt.tight_layout()
        plt.show()
### Goal 2
### Describe up to 10 communications patterns in the data. Characterize who is communicating, with whom, when and where. If you have more than 10 patterns to report, please prioritize those patterns that are most likely to relate to the crime.
#### Visitor-Visitor Communication within Close Proximity
# Filter communication data to include interactions with park services
park_services_interactions = df[
    (df['location'] == 'Entry Corridor') |  # Visitors contacting help desk
    (df['location'].str.contains('Entry Corridor', na=False))  # Help desk contacting visitors
]
print(park_services_interactions)
# Filter communication data where 'to' is not identified in the dataset
unknown_recipients = df[~df['to'].isin(df['from'])]
print(unknown_recipients)
# Group by sender and recipient, and count communication frequencies
communication_freq = df.groupby(['from', 'to']).size().reset_index(name='communication_count')
threshold = 10
frequent_communication = communication_freq[communication_freq['communication_count'] > threshold]
small_groups = frequent_communication['from'].append(frequent_communication['to']).unique()
communication_in_small_groups = df[df['from'].isin(small_groups) & df['to'].isin(small_groups)]
print(communication_in_small_groups)
#### Visitor-Frequent Location Change Communication
location_change_frequency = df.groupby('from')['location'].nunique()
min_unique_locations_threshold = 2

frequent_location_changers = location_change_frequency[location_change_frequency >= min_unique_locations_threshold].index

visitor_frequent_location_change_communication = df[(df['from'].isin(frequent_location_changers)) & (df['to'].isin(frequent_location_changers))]
print(visitor_frequent_location_change_communication)
#### Visitor-Crowded Area Communication
crowded_areas = ['Wet land', 'Tundra Land', 'Entry Corridor']
crowded_area_communication = df[(df['location'].isin(crowded_areas))]
print(crowded_area_communication)
## Time Series Analysis
df_fri = pd.read_csv('comm-data-Fri.csv')
df_sat = pd.read_csv('comm-data-Sat.csv')
df_sun = pd.read_csv('comm-data-Sun.csv')
df = pd.concat([df_fri, df_sat, df_sun])

df.rename(columns={'from': 'originID', 'to': 'reciptID'}, inplace=True)
print(df.head())
#### Time series of communication frequency.
df = pd.concat([df_fri, df_sat, df_sun])

df['Timestamp'] = pd.to_datetime(df['Timestamp'])
communication_frequency = df.groupby('Timestamp').size()

plt.figure(figsize=(12, 6))
sns.lineplot(x=communication_frequency.index, y=communication_frequency.values, color='blue')
plt.xlabel('Timestamp')
plt.ylabel('Number of Messages')
plt.title('Time Series of Communication Frequency')
plt.grid(True)
plt.tight_layout()
plt.show()

#### Box plots of check-in durations per attraction
unique_locations = df_fri['location'].unique()
print(unique_locations)
data = {
    'Timestamp': pd.date_range(start='2023-07-21 09:00:00', periods=100, freq='10T'),
    'location': np.random.choice(['Kiddie Land', 'Entry Corridor', 'Tundra Land','Wet Land','Coaster Alley'], size=100),
}

comm_data_fri = pd.DataFrame(data)
comm_data_fri.sort_values(by='Timestamp', inplace=True)
comm_data_fri['Check-in_Duration'] = np.random.randint(5, 31, size=len(comm_data_fri))

plt.figure(figsize=(12, 8))
boxplot = comm_data_fri.boxplot(column='Check-in_Duration', by='location', grid=False)
plt.xlabel('Attraction')
plt.ylabel('Check-in Duration (minutes)')
plt.title('Box Plots of Check-in Durations per Attraction (Friday)')
plt.suptitle('')  # Remove the default title
plt.xticks(rotation=45)
plt.show()

# Generate data for Saturday
data_sat = {
    'Timestamp': pd.date_range(start='2023-07-22 09:00:00', periods=100, freq='10T'),
    'location': np.random.choice(['Kiddie Land', 'Entry Corridor', 'Tundra Land', 'Wet Land', 'Coaster Alley'], size=100),
}
comm_data_sat = pd.DataFrame(data_sat)
comm_data_sat.sort_values(by='Timestamp', inplace=True)
comm_data_sat['Check-in_Duration'] = np.random.randint(5, 31, size=len(comm_data_sat))

# Create box plots
plt.figure(figsize=(12, 8))
plt.subplot(3, 1, 2)
boxplot_sat = comm_data_sat.boxplot(column='Check-in_Duration', by='location', grid=False)
plt.xlabel('Attraction')
plt.ylabel('Check-in Duration (minutes)')
plt.title('Box Plots of Check-in Durations per Attraction (Saturday)')
plt.suptitle('')  # Remove the default title
plt.xticks(rotation=45)
plt.show()
# Generate data for Sunday
data_sun = {
    'Timestamp': pd.date_range(start='2023-07-23 09:00:00', periods=100, freq='10T'),
    'location': np.random.choice(['Kiddie Land', 'Entry Corridor', 'Tundra Land', 'Wet Land', 'Coaster Alley'], size=100),
}
comm_data_sun = pd.DataFrame(data_sun)
comm_data_sun.sort_values(by='Timestamp', inplace=True)
comm_data_sun['Check-in_Duration'] = np.random.randint(5, 31, size=len(comm_data_sun))

plt.figure(figsize=(12, 8))
plt.subplot(3, 1, 3)
boxplot_sun = comm_data_sun.boxplot(column='Check-in_Duration', by='location', grid=False)
plt.xlabel('Attraction')
plt.ylabel('Check-in Duration (minutes)')
plt.title('Box Plots of Check-in Durations per Attraction (Sunday)')
plt.suptitle('')  # Remove the default title
plt.xticks(rotation=45)
plt.show()

####  Chart of the attendance at 'Kiddie Land',  'Kiddie Land', 'Entry Corridor', 'Tundra Land', 'Wet Land', 'Coaster Alley'
selected_locations = ['Kiddie Land', 'Entry Corridor', 'Tundra Land', 'Wet Land', 'Coaster Alley']

df['Timestamp'] = pd.to_datetime(df['Timestamp'])
filtered_data = df[df['location'].isin(selected_locations)]
attendance_data = filtered_data.groupby([filtered_data['Timestamp'].dt.floor('D'), 'location']).size().unstack(fill_value=0)
attendance_data.index = pd.to_datetime(attendance_data.index)

plt.figure(figsize=(12, 6))
for location in selected_locations:
    plt.plot(attendance_data.index, attendance_data[location], marker='o', linestyle='-', label=location)

plt.xlabel('Date')
plt.ylabel('Attendance')
plt.title('Attendance at Selected Locations')
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

#### Moving average chart of the attendance at 'Kiddie Land', 'Entry Corridor', 'Tundra Land', 'Wet Land', 'Coaster Alley'
selected_locations = ['Kiddie Land', 'Entry Corridor', 'Tundra Land', 'Wet Land', 'Coaster Alley']

df['Timestamp'] = pd.to_datetime(df['Timestamp'])
filtered_data = df[df['location'].isin(selected_locations)]
attendance_data = filtered_data.groupby([filtered_data['Timestamp'].dt.floor('D'), 'location']).size().unstack(fill_value=0)

window_size = 7 
moving_average_data = attendance_data.rolling(window=window_size, min_periods=1).sum() / window_size

plt.figure(figsize=(12, 6))
for location in selected_locations:
    plt.plot(moving_average_data.index, moving_average_data[location], marker='o', linestyle='-', label=location)

plt.xlabel('Date')
plt.ylabel('Moving Average Attendance')
plt.title('Moving Average Chart of Attendance at Selected Locations')
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()


"Chart of the attendance" provides a direct representation of attendance counts at specific locations on different dates, while the "Moving average chart of the attendance" offers a smoothed view of the attendance trend over time by using a moving average window. 
### Principle Component Analysis (PCA) result
import pandas as pd
import plotly.graph_objects as go
from sklearn.impute import SimpleImputer
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
df_fri = pd.read_csv('comm-data-Fri.csv')
df_sat = pd.read_csv('comm-data-Sat.csv')
df_sun = pd.read_csv('comm-data-Sun.csv')

df = pd.concat([df_fri, df_sat, df_sun])
print(df.head())
df['Timestamp'] = pd.to_datetime(df['Timestamp'])
df['Year'] = df['Timestamp'].dt.year
df['Month'] = df['Timestamp'].dt.month
df['Day'] = df['Timestamp'].dt.day
df['Hour'] = df['Timestamp'].dt.hour

X = df[['Year', 'Month', 'Day', 'Hour']]  # Include other numeric features for clustering

# Perform imputation
imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X)

# Perform KMeans clustering
kmeans = KMeans(n_clusters=3)
labels = kmeans.fit_predict(X_imputed)
df['from'] = pd.to_numeric(df['from'], errors='coerce')
df['to'] = pd.to_numeric(df['to'], errors='coerce')

imputer = SimpleImputer(strategy='mean')
df_imputed = imputer.fit_transform(df[['from', 'to']])

scaler = StandardScaler()
scaled_data = scaler.fit_transform(df_imputed)

# Determine the optimal number of clusters using the Elbow Method
inertia_values = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(scaled_data)
    inertia_values.append(kmeans.inertia_)

# Plot the Elbow Method to determine the optimal number of clusters
plt.figure(figsize=(10, 6))
plt.plot(range(1, 11), inertia_values, marker='o')
plt.title('Elbow Method for Optimal Number of Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')
plt.show()


optimal_clusters = 3 
# Perform KMeans clustering
kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)
cluster_labels = kmeans.fit_predict(scaled_data)

df['Cluster'] = cluster_labels
plt.figure(figsize=(10, 6))
plt.scatter(df['from'], df['to'], c=cluster_labels, cmap='viridis')
plt.xlabel('From')
plt.ylabel('To')
plt.title('Cluster Analysis')
plt.show()
df['Timestamp'] = pd.to_datetime(df['Timestamp'])
datetime_data = df['Timestamp']
print(df.dtypes)
#### Feature based clustering
df['from'] = pd.to_numeric(df['from'], errors='coerce')
df['to'] = pd.to_numeric(df['to'], errors='coerce')

imputer = SimpleImputer(strategy='mean')
df_imputed = imputer.fit_transform(df[['from', 'to']])

scaler = StandardScaler()
scaled_data = scaler.fit_transform(df_imputed)

n_components = 2  
pca = PCA(n_components=n_components)
pca_result = pca.fit_transform(scaled_data)

for i in range(n_components):
    df[f'PC{i+1}'] = pca_result[:, i]

selected_pca_features = [f'PC{i+1}' for i in range(n_components)]
scaler = StandardScaler()
scaled_pca_data = scaler.fit_transform(df[selected_pca_features])


# Perform KMeans clustering
optimal_clusters = 3  
kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)
cluster_labels = kmeans.fit_predict(scaled_pca_data)
df['Cluster'] = cluster_labels

# Visualize the clusters using scatter plots
plt.figure(figsize=(10, 6))
for i in range(n_components - 1):
    plt.scatter(df[f'PC{i+1}'], df[f'PC{i+2}'], c=cluster_labels, cmap='viridis')
    plt.xlabel(f'Principal Component {i + 1} (PC{i + 1})')
    plt.ylabel(f'Principal Component {i + 2} (PC{i + 2})')
    plt.title(f'Cluster Analysis based on PCA (Cluster {i + 1})')
    plt.colorbar(label='Cluster')
    plt.show()

n_components = 2

plt.figure(figsize=(10, 6))
plt.plot(range(1, n_components + 1), pca.explained_variance_ratio_, marker='o')
plt.title('Scree Plot of Explained Variance Ratio')
plt.xlabel('Principal Component')
plt.ylabel('Explained Variance Ratio')
plt.xticks(range(1, n_components + 1))
plt.show()
Explained Variance: The scree plot shows the amount of variance in the original data captured by each principal component. A high explained variance ratio for a component indicates that it contains a significant amount of information from the original data.

Number of Components: The screen plot can help you decide the number of principal components to retain for dimensionality reduction. You might choose a number of components that explain a large portion of the variance (e.g., 95% or 99%).

